{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_type=\"azure\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/LangChain/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "code = {\n",
    "    \"template\": \"You are a developer. Write a program in {language} that does {task}.\",\n",
    "    \"input\": [\"language\", \"task\"],\n",
    "    \"output\": \"code\"\n",
    "} \n",
    "explain = {\n",
    "    \"template\": \"Explain this {code}.\",\n",
    "    \"input\": [\"code\"],\n",
    "    \"output\": \"explaination\"\n",
    "}\n",
    "document = {\n",
    "    \"template\": \"Create a markdown document for this {explaination}.\",\n",
    "    \"input\": [\"explaination\"],\n",
    "    \"output\": \"document\"\n",
    "}\n",
    "\n",
    "chains = []\n",
    "outputs = []\n",
    "for task in [code, explain, document]:\n",
    "    prompt_template = PromptTemplate(input_variable = task[\"input\"], template = task[\"template\"])\n",
    "    task_chain = LLMChain(\n",
    "        llm = model,\n",
    "        prompt = prompt_template,\n",
    "        output_key = task[\"output\"]\n",
    "    )\n",
    "    outputs.append(task[\"output\"])\n",
    "    chains.append(task_chain)\n",
    "seq_chain = SequentialChain(chains=chains, input_variables=code[\"input\"], output_variables=outputs)\n",
    "results = seq_chain({\n",
    "    \"language\": \"Python\",\n",
    "    \"task\": \"function that calculate 3 dimentional gradient descent.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the main difference between `SimpleSequentialChain` and `SequentialChain`, `SequentialChain` can takes in multiple input and provide multiple output with one go, below you can see from the `results`, you can unpack it to multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'Python', 'task': 'function that calculate 3 dimentional gradient descent.', 'code': 'Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or the approximate gradient) of the function at the current point.\\n\\nBelow is a Python program that calculates the gradient descent in 3 dimensions.\\n\\n```python\\nimport numpy as np\\n\\n# The function to optimize\\ndef f(x, y, z):\\n    return x**2 + y**2 + z**2\\n\\n# The gradient of the function\\ndef gradient(x, y, z):\\n    return np.array([2*x, 2*y, 2*z])\\n\\n# Gradient descent\\ndef gradient_descent(starting_point=None, iterations=10, learning_rate=1):\\n    if starting_point is None:\\n        point = np.random.uniform(-10, 10, size=3)\\n    else:\\n        point = starting_point\\n    trajectory = [point]\\n    \\n    for iteration in range(iterations):\\n        grad = gradient(*point)\\n        point = point - learning_rate * grad\\n        trajectory.append(point)\\n    \\n    return np.array(trajectory)\\n\\n# Test the function\\ntrajectory = gradient_descent(iterations=100, learning_rate=0.1)\\n\\nprint(\"Minima found at:\")\\nprint(trajectory[-1])\\n\\nprint(\"Value at this minima:\")\\nprint(f(*trajectory[-1]))\\n```\\n\\nThis program first defines the function to optimize and its gradient. The gradient descent algorithm starts at a random point, and for a number of iterations, it moves in the direction of the negative gradient of the function at that point. The distance it moves is proportional to the learning rate.\\n\\nThe algorithm keeps track of the trajectory of points it has visited, and returns this trajectory at the end. You can see the point at which it found a minima, and the value of the function at that minima.\\n\\nThis particular function has a global minimum at `(0, 0, 0)`, and this is where gradient descent will converge to, given enough iterations and a suitable learning rate.', 'explaination': \"Gradient descent is an optimization algorithm used for finding the minimum of a function. It is based on the observation that if a multi-variable function is defined and differentiable in a neighborhood of a point, then the function decreases fastest if one goes from this point in the direction of the negative gradient of the function at that point.\\n\\nIn the provided Python program, the function being optimized is `f(x, y, z) = x^2 + y^2 + z^2` and its gradient is `gradient(x, y, z) = [2x, 2y, 2z]`. The gradient is a vector that points in the direction of the greatest rate of increase of the function, and its magnitude is the rate of increase in that direction.\\n\\nThe gradient descent algorithm starts from a random point in 3D space (or a user-specified starting point), and iteratively moves in the direction of the negative gradient (i.e., the direction of decrease) of the function at the current point. The size of each step is determined by the learning rate: a higher learning rate makes larger steps and converges faster, but risks overshooting the minimum; a smaller learning rate makes smaller steps and converges slower, but provides more precise convergence to the minimum.\\n\\nThe algorithm stores all points visited during the iterations (the trajectory), and these are returned at the end of the function. The last point in the trajectory is the point where the algorithm found a minimum, and the value of the function at that point is the function's minimum value.\\n\\nSince the function `f(x, y, z) = x^2 + y^2 + z^2` has a global minimum at `(0, 0, 0)`, given enough iterations and an appropriate learning rate, the gradient descent algorithm will converge to this point.\", 'document': \"# Gradient Descent: An Optimization Algorithm\\n\\nGradient descent is an optimization algorithm utilized for finding the minimum of a function. This algorithm is based on the observation that if a multi-variable function is defined and differentiable in a neighborhood of a point, the function decreases fastest if one goes from this point in the direction of the negative gradient of the function at that point.\\n\\n## Provided Python Program\\n\\nIn the provided Python program, the function being optimized is:\\n\\n```\\nf(x, y, z) = x^2 + y^2 + z^2 \\n```\\n\\nand its gradient is:\\n\\n```\\ngradient(x, y, z) = [2x, 2y, 2z]\\n```\\n\\nThe gradient is a vector that points in the direction of the greatest rate of increase of the function, and its magnitude is the rate of increase in that direction.\\n\\n## The Gradient Descent Algorithm\\n\\nThe gradient descent algorithm starts from a random point in 3D space (or a user-specified starting point), and iteratively moves in the direction of the negative gradient (i.e., the direction of decrease) of the function at the current point. The size of each step is determined by the learning rate. \\n\\nA higher learning rate makes larger steps and converges faster, but risks overshooting the minimum. A smaller learning rate makes smaller steps and converges slower, but provides more precise convergence to the minimum.\\n\\nThe algorithm stores all points visited during the iterations (the trajectory), and these are returned at the end of the function. The last point in the trajectory is the point where the algorithm found a minimum, and the value of the function at that point is the function's minimum value.\\n\\n## Conclusion\\n\\nSince the function `f(x, y, z) = x^2 + y^2 + z^2` has a global minimum at `(0, 0, 0)`, given enough iterations and an appropriate learning rate, the gradient descent algorithm will converge to this point.\"}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or the approximate gradient) of the function at the current point.\n",
      "\n",
      "Below is a Python program that calculates the gradient descent in 3 dimensions.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# The function to optimize\n",
      "def f(x, y, z):\n",
      "    return x**2 + y**2 + z**2\n",
      "\n",
      "# The gradient of the function\n",
      "def gradient(x, y, z):\n",
      "    return np.array([2*x, 2*y, 2*z])\n",
      "\n",
      "# Gradient descent\n",
      "def gradient_descent(starting_point=None, iterations=10, learning_rate=1):\n",
      "    if starting_point is None:\n",
      "        point = np.random.uniform(-10, 10, size=3)\n",
      "    else:\n",
      "        point = starting_point\n",
      "    trajectory = [point]\n",
      "    \n",
      "    for iteration in range(iterations):\n",
      "        grad = gradient(*point)\n",
      "        point = point - learning_rate * grad\n",
      "        trajectory.append(point)\n",
      "    \n",
      "    return np.array(trajectory)\n",
      "\n",
      "# Test the function\n",
      "trajectory = gradient_descent(iterations=100, learning_rate=0.1)\n",
      "\n",
      "print(\"Minima found at:\")\n",
      "print(trajectory[-1])\n",
      "\n",
      "print(\"Value at this minima:\")\n",
      "print(f(*trajectory[-1]))\n",
      "```\n",
      "\n",
      "This program first defines the function to optimize and its gradient. The gradient descent algorithm starts at a random point, and for a number of iterations, it moves in the direction of the negative gradient of the function at that point. The distance it moves is proportional to the learning rate.\n",
      "\n",
      "The algorithm keeps track of the trajectory of points it has visited, and returns this trajectory at the end. You can see the point at which it found a minima, and the value of the function at that minima.\n",
      "\n",
      "This particular function has a global minimum at `(0, 0, 0)`, and this is where gradient descent will converge to, given enough iterations and a suitable learning rate.\n"
     ]
    }
   ],
   "source": [
    "print(results[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent is an optimization algorithm used for finding the minimum of a function. It is based on the observation that if a multi-variable function is defined and differentiable in a neighborhood of a point, then the function decreases fastest if one goes from this point in the direction of the negative gradient of the function at that point.\n",
      "\n",
      "In the provided Python program, the function being optimized is `f(x, y, z) = x^2 + y^2 + z^2` and its gradient is `gradient(x, y, z) = [2x, 2y, 2z]`. The gradient is a vector that points in the direction of the greatest rate of increase of the function, and its magnitude is the rate of increase in that direction.\n",
      "\n",
      "The gradient descent algorithm starts from a random point in 3D space (or a user-specified starting point), and iteratively moves in the direction of the negative gradient (i.e., the direction of decrease) of the function at the current point. The size of each step is determined by the learning rate: a higher learning rate makes larger steps and converges faster, but risks overshooting the minimum; a smaller learning rate makes smaller steps and converges slower, but provides more precise convergence to the minimum.\n",
      "\n",
      "The algorithm stores all points visited during the iterations (the trajectory), and these are returned at the end of the function. The last point in the trajectory is the point where the algorithm found a minimum, and the value of the function at that point is the function's minimum value.\n",
      "\n",
      "Since the function `f(x, y, z) = x^2 + y^2 + z^2` has a global minimum at `(0, 0, 0)`, given enough iterations and an appropriate learning rate, the gradient descent algorithm will converge to this point.\n"
     ]
    }
   ],
   "source": [
    "print(results[\"explaination\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Gradient Descent: An Optimization Algorithm\n",
      "\n",
      "Gradient descent is an optimization algorithm utilized for finding the minimum of a function. This algorithm is based on the observation that if a multi-variable function is defined and differentiable in a neighborhood of a point, the function decreases fastest if one goes from this point in the direction of the negative gradient of the function at that point.\n",
      "\n",
      "## Provided Python Program\n",
      "\n",
      "In the provided Python program, the function being optimized is:\n",
      "\n",
      "```\n",
      "f(x, y, z) = x^2 + y^2 + z^2 \n",
      "```\n",
      "\n",
      "and its gradient is:\n",
      "\n",
      "```\n",
      "gradient(x, y, z) = [2x, 2y, 2z]\n",
      "```\n",
      "\n",
      "The gradient is a vector that points in the direction of the greatest rate of increase of the function, and its magnitude is the rate of increase in that direction.\n",
      "\n",
      "## The Gradient Descent Algorithm\n",
      "\n",
      "The gradient descent algorithm starts from a random point in 3D space (or a user-specified starting point), and iteratively moves in the direction of the negative gradient (i.e., the direction of decrease) of the function at the current point. The size of each step is determined by the learning rate. \n",
      "\n",
      "A higher learning rate makes larger steps and converges faster, but risks overshooting the minimum. A smaller learning rate makes smaller steps and converges slower, but provides more precise convergence to the minimum.\n",
      "\n",
      "The algorithm stores all points visited during the iterations (the trajectory), and these are returned at the end of the function. The last point in the trajectory is the point where the algorithm found a minimum, and the value of the function at that point is the function's minimum value.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Since the function `f(x, y, z) = x^2 + y^2 + z^2` has a global minimum at `(0, 0, 0)`, given enough iterations and an appropriate learning rate, the gradient descent algorithm will converge to this point.\n"
     ]
    }
   ],
   "source": [
    "print(results[\"document\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
