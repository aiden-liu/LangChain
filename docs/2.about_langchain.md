### Prompt Engineering
| Paper: [Lost in the Middle: How Language Model Use Long Contexts](https://arxiv.org/pdf/2307.03172). <br/>
| Framework: [CO-STAR](https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875)

1. [First Experiment](../labs/1.first_langchain_programe.ipynb)
1. Zero/One/Few shot learning, by providing no/one/few examples for the LLM to figure out the answer. [lab](../labs/4.prompt_engineering.ipynb)
1. [LangChain prompt template](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates)
1. Output Parsers [[lab]](../labs/5.output_parser.ipynb)
   * get_format_instructions()
   * parse()
   * parse_with_prompt
1. Serialization [[lab]](../labs/6.serialization.ipynb)
1. Summarize [[lab]](../labs/7.tweet_creator.ipynb)

### Core Concepts

### Memory
Types of Memory Classes
1. ConversationBufferMemory
1. ConversationBufferWindow
1. ConversationTokenBufferMemory
1. ConversationSummaryMemory

### Chain
1. LLMChain [[lab]](../labs/9.llm_chain.ipynb), consists of two parts:
   * PromptTemplate
   * Output parser
1. SimpleSequentialChain [[labs]](../labs/10.simple_sequential_chain.ipynb)
1. SequentialChain [[lab]](../labs/11.sequential_chain.ipynb)

### LangSmith
This was launched in July 2023 and is currently in closed beta? Basically is a platform that allows for creating LLM-based application for production.

| Documentation for LangSmith can be found [here](https://docs.smith.langchain.com)

It is seamlessly integrated with LangChain, however it can be use for other programmes written in different frameworks. An LLM-based application has a stochastic nature, meaning the outputs are based on probabilities. So this makes it difficult to test and evaluate the application. LangSmith's core is dealing with this. Capabilites include:
1. Debugging traces inputs and outputs in the model for every step in the chain. This helps with evaluating the application and experimenting with new chains, like errors and latency.
2. Testing against changes with prompts and chains, and how they affect the outputs.
3. Evaluating using open-source evaluation models, first approach is heuristics, like regex to evaluate an answer, second is LLM, meaning use LLM to evaluate themselves.
4. Monitoring to track latency, cost and errors.

Labs in this project are already have LangSmith integrated, and details are available at: https://smith.langchain.com/